{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lecture 4: vector similarity\n",
    "===============\n",
    "\n",
    "9/17/2024, CS 4120 Natural Language Processing, ElSherief\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1: Calculating tf-idf\n",
    "----\n",
    "\n",
    "To calculate tf-idf, we'll need to first construct a term-document matrix from our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for tokenization, not necessary\n",
    "# (comment out if you don't have nltk installed yet)\n",
    "import nltk\n",
    "\n",
    "# helpful for calculating log10 and because\n",
    "# numpy arrays make certain manipulations\n",
    "# (like getting a column of numbers)\n",
    "# easier\n",
    "import numpy as np\n",
    "\n",
    "# useful for counting :)\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x9d in position 6229: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 51\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;66;03m# if you don't have nltk installed, use another tokenization\u001b[39;00m\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;66;03m# strategy here like str.split()\u001b[39;00m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m nltk\u001b[38;5;241m.\u001b[39mword_tokenize(contents)\n\u001b[1;32m---> 51\u001b[0m mobydick \u001b[38;5;241m=\u001b[39m load_tokens(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmobydick.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     52\u001b[0m shakes \u001b[38;5;241m=\u001b[39m load_tokens(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshakesdown.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     53\u001b[0m pandp \u001b[38;5;241m=\u001b[39m load_tokens(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprideandprejudice.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[4], line 45\u001b[0m, in \u001b[0;36mload_tokens\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_tokens\u001b[39m(filename):\n\u001b[0;32m     44\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 45\u001b[0m     contents \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39mlower()\n\u001b[0;32m     46\u001b[0m     f\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;66;03m# if you don't have nltk installed, use another tokenization\u001b[39;00m\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;66;03m# strategy here like str.split()\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\encodings\\cp1252.py:23\u001b[0m, in \u001b[0;36mIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m---> 23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m codecs\u001b[38;5;241m.\u001b[39mcharmap_decode(\u001b[38;5;28minput\u001b[39m,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors,decoding_table)[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x9d in position 6229: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "def tf(term: str, document: list) -> float:\n",
    "    \"\"\"\n",
    "    Calculate term frequency\n",
    "    Parameters:\n",
    "    term - string\n",
    "    document - list of strings (tokenized document)\n",
    "    Return:\n",
    "    float term frequency\n",
    "    \"\"\"\n",
    "    return np.log10(document.count(term) + 1)\n",
    "\n",
    "def idf(N: int, df_t: int) -> float:\n",
    "    \"\"\"\n",
    "    Calculate inverse document frequency given the \n",
    "    number of documents and the number of documents the term appears in.\n",
    "    Paramenters:\n",
    "    N - int (number of documents)\n",
    "    df_t - int (number of documents the term appears in)\n",
    "    Return:\n",
    "    float inverse document frequency\n",
    "    \"\"\"\n",
    "    return np.log10(N / df_t)\n",
    "\n",
    "\n",
    "def term_in_documents(term: str, documents: list) -> int:\n",
    "    \"\"\"\n",
    "    Calculate the number of documents in a list of documents that a target\n",
    "    term appears in.\n",
    "    Parameters:\n",
    "    term - str\n",
    "    documents - list of list of str (list of tokenized documents)\n",
    "    Return:\n",
    "    int number of documents the term appears in\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    for document in documents:\n",
    "        if term in document:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "# load in the data\n",
    "def load_tokens(filename):\n",
    "    f = open(filename, 'r')\n",
    "    contents = f.read().lower()\n",
    "    f.close()\n",
    "    # if you don't have nltk installed, use another tokenization\n",
    "    # strategy here like str.split()\n",
    "    return nltk.word_tokenize(contents)\n",
    "\n",
    "mobydick = load_tokens('mobydick.txt')\n",
    "shakes = load_tokens('shakesdown.txt')\n",
    "pandp = load_tokens('prideandprejudice.txt')\n",
    "books = [mobydick, shakes, pandp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we won't create the entire term-document matrix, we'll just do it for a few key terms that we\n",
    "# care about for the sake of time\n",
    "# TODO: pick 3 - 5 words\n",
    "words = ['your words here']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the term-document matrix\n",
    "matrix = []\n",
    "\n",
    "# iterate through your chosen words\n",
    "for word in words:\n",
    "    # count how many times it occurs in each book\n",
    "    # this is just for debugging for you\n",
    "    counts = [str(b.count(word)) for b in books]\n",
    "    print(word, \":\\t\", \"\\t\".join(counts))\n",
    "    \n",
    "    \n",
    "    # calculate the term frequency for each book\n",
    "    # for this word\n",
    "    # tf_words should be the same length as counts\n",
    "#     tf_words = YOUR CODE HERE\n",
    "    \n",
    "    # calculate idf for this term\n",
    "    # this will be a single scalar\n",
    "    N = len(books)\n",
    "    df_t = term_in_documents(word, books)\n",
    "#     idf_t = YOUR CODE HERE\n",
    "    \n",
    "    # multiply tf with idf for each book/each\n",
    "    # term frequency you calculated\n",
    "#     tfidf_words = YOUR CODE HERE\n",
    "    \n",
    "    # add the tfidf numbers to your matrix\n",
    "    matrix.append(tfidf_words)\n",
    "    \n",
    "    # uncomment to see visually the different components (helpful for debugging)\n",
    "#     print(word, \" tf:\\t\", \"\\t\".join([str(x) for x in tf_words]))\n",
    "#     print(word, \"idf:\\t\", idf_t)\n",
    "#     print(word, \" tf-idf:\\t\", \"\\t\".join([str(x) for x in tfidf_words]))\n",
    "    \n",
    "# if you'd like to, uncomment the following code to make the matrix into a numpy array\n",
    "# matrix = np.array(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What should the dimensions of your matrix be? __number of words (5) by number of books (3)__\n",
    "2. What happens if you attempt to calculate tfidf of a term that exists in *none* of your books? __Error, unless you adjust the code__\n",
    "2. What happens if you attempt to calculate tfidf of a term that exists in *all* of your books? __it's zero!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the dimensions of your matrix\n",
    "# number of rows should match number of words\n",
    "# number of cols should match number of documents (books)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(v1: list, v2: list) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the cosine similarity between two vectors of the same length.\n",
    "    Parameters:\n",
    "    v1 - list (of numbers)\n",
    "    v2 - list (of numbers)\n",
    "    Return:\n",
    "    float cosine similarity\n",
    "    \"\"\"\n",
    "    # you may find the numpy functions np.dot() and np.linalg.norm() useful\n",
    "    # TODO: implement\n",
    "    return 0\n",
    "\n",
    "# calculate the similarity between two *word* vectors\n",
    "# we'll just do word vectors because unless matrix is a numpy array\n",
    "# it is (more) difficult to get column vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you finish, which book is closest to moby dick?\n",
    "# you'll need a *column* vector here (instead of a row vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you finish finish, which book is closest to moby dick, but remake your matrix with all vocabulary terms?\n",
    "# create the term-document matrix\n",
    "# you'll want to use counters for each book's vocabulary for the sake of efficiency\n",
    "\n",
    "\n",
    "# you may want to re-define new counter versions of your tf, idf, term_in_documents \n",
    "# functions so that they work with counters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify the shape of your matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which book is actually closest to moby dick?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2: install `nltk` (this is the same task 2 from lecture 3)\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you finish the first task, work on making sure that you have `nltk` downloaded and accessible to your jupyter notebooks. While you will not be allowed to use `nltk` for *most* of your homework, we will use it frequently in class to demonstrate tools. \n",
    "\n",
    "[`nltk`](https://www.nltk.org/) (natural language toolkit) is a python package that comes with many useful implementations of NLP tools and datasets.\n",
    "\n",
    "From the command line, using pip: `pip3 install nltk`\n",
    "\n",
    "[installing nltk](https://www.nltk.org/install.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# the stemmer we'll use\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# also grab a lemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# for the tokenizer that we're going to use\n",
    "# won't cause an error if you've already downloaded it\n",
    "nltk.download('punkt')\n",
    "# for the lemmatizer\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"N.K. Jemison is a science fiction author.\"\n",
    "words = nltk.word_tokenize(example)\n",
    "\n",
    "# not perfect, but much better\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the nltk tokenizer, tokenize Moby Dick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. How many tokens do you have now? how big is the vocabulary? Are these numbers larger or smaller than using `str.split()` to tokenize? __YOUR ANSWER HERE__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "# example stemming at the individual token level\n",
    "for w in moby_nltk_tokens[:5]:\n",
    "    print(porter_stemmer.stem(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. How big is the vocabulary of stems? __YOUR ANSWER HERE__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many lemmas in the vocabulary?\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "# the lemmatizer works using the method .lemmatize(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. How big is the vocabulary of lemmas? __YOUR ANSWER HERE__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. make a graph of heap's law with separate series for tokens, stems, and lemmas. Do they follow the same patterns? __YOUR ANSWER HERE__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
