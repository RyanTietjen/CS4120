{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: Classification! (and some n-gram math)\n",
    "\n",
    "#### Due on 02/02/2024 @ 21:00 hours\n",
    "\n",
    "Agenda\n",
    "------\n",
    "+ Detecting the end of a sentence\n",
    "    - Rule-based classifier\n",
    "+ Detecting the sentiment of a sentence\n",
    "    - Rule-based classifier (counting words)\n",
    "    - Measuring Accuracy, Precision, Recall (evaluating a classifier)\n",
    "+ N-gram Math (getting started on things for HW 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking ahead, we'll be focusing on *classification* for much of the next several weeks. Classification can take several forms. Here are some vocabulary terms to get you started:\n",
    "\n",
    "- __classifier__: a model that takes data (text, in NLP) as input and outputs a category\n",
    "- __binary classification__: a model that takes input and outputs *one of two* categories (e.g. \"positive\" or \"negative\")\n",
    "- __multinomial classification__: a model that takes input and outputs *one of many* categories (e.g. \"positive\", \"neutral\" or \"negative\" or a language model that chooses one token from the entire vocabulary)\n",
    "\n",
    "\n",
    "- __rule-based classifier__: a classifier that functions based on rules that humans come up with (e.g. \"the end of a sentence is when there is a \".\" \")\n",
    "- __statistical classifier__: a classifier that functions based on counts (statistics) that it has gathered or based on running an algorithm to automatically train parameters on a given data set. \n",
    "    \n",
    "In this lab, you'll be building rule-based classifiers and evaluating them. We'll learn about our first statistical classifier next lecture\n",
    "\n",
    "All tasks have equal weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 0: Who is in your group?\n",
    "\n",
    "__Ryan Tietjen__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Detecting the end of a sentence\n",
    "\n",
    "\n",
    "A classifier is, in essence, a function that takes some data $x$ and assigns some label $y$ to it. For a binary classifier, we can model this a function that takes a data point $x$ and returns either `True` or `False`.\n",
    "\n",
    "Later in this class we'll learn about how to build classifiers that automatically learn how to do this, but we'll start where NLP startedâ€”writing some rule-based classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_sentence_end(text: str, target_index: int) -> bool: \n",
    "    \"\"\"\n",
    "    Classify whether or not a *location* is the end of a sentence within\n",
    "    a given text\n",
    "    Parameters:\n",
    "        text - string piece of text\n",
    "        target_index - int candidate location\n",
    "    returns true if the target index is the end of a sentence. \n",
    "    False otherwise. \n",
    "    \"\"\"\n",
    "    if target_index == len(text) - 1:\n",
    "        return True\n",
    "    elif text[target_index] == \".\" and text[target_index+1] == \" \":\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# look at the code in the cell below to see example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stocks were up as advancing issues outpaced declining issues on the NYSE by 1.5 to 1.\n",
      "****\n",
      " Large- and small-cap stocks were both strong, while the S.&P.\n",
      "****\n",
      " 500 index gained 0.46% to finish at 2,457.59.\n",
      "****\n",
      " Among individual stocks, the two top percentage gainers in the S.&P.\n",
      "****\n",
      " 500 were Incyte Corporation and Gilead Sciences Inc.\n",
      "****\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# example text\n",
    "# feel free to go through different examples\n",
    "\n",
    "# This is the given example text\n",
    "\"\"\"Stocks were up as advancing issues outpaced declining issues \n",
    "          on the NYSE by 1.5 to 1. Large- and small-cap stocks were both strong, \n",
    "          while the S.&P. 500 index gained 0.46% to finish at 2,457.59. Among \n",
    "          individual stocks, the two top percentage gainers in the S.&P. 500 \n",
    "          were Incyte Corporation and Gilead Sciences Inc.\"\"\"\n",
    "\n",
    "example = \"Stocks were up as advancing issues outpaced declining issues on the NYSE by 1.5 to 1. Large- and small-cap stocks were both strong, while the S.&P. 500 index gained 0.46% to finish at 2,457.59. Among individual stocks, the two top percentage gainers in the S.&P. 500 were Incyte Corporation and Gilead Sciences Inc.\"\n",
    "\n",
    "# this code will go through and\n",
    "# build up a string based on the sentence\n",
    "# decisions that your classifier comes up with\n",
    "# it will put \"****\" between the sentences\n",
    "# you do not need to modify any code here\n",
    "so_far = \"\"\n",
    "for index in range(len(example)):\n",
    "    # see how the classify_sentence_end function is called!\n",
    "    result = classify_sentence_end(example, index)\n",
    "    so_far += example[index]\n",
    "    if result:\n",
    "        print(so_far)\n",
    "        print(\"****\")\n",
    "        so_far = \"\"\n",
    "        \n",
    "print(so_far)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. How many sentences are detected using your end of sentence classifier? **5**\n",
    "2. Where did your end of sentence classifier make a mistake? **The classifier mistakenly classifies 'S.&P.' as the end of a sentence because it ends with \". \"**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2: Determining Sentiment\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll use nltk to access the reviews that we want to classify eventually\n",
    "import nltk\n",
    "import nltk.corpus as corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_word_list(filename):\n",
    "    \"\"\"\n",
    "    Loads a lexicon from a plain text file in the format of one word per line.\n",
    "    Parameters:\n",
    "    filename (str): path to file\n",
    "\n",
    "    Returns:\n",
    "    list: list of words\n",
    "    \"\"\"\n",
    "    with open(filename, 'r', encoding=\"utf-8\") as f:\n",
    "        # skip the header content\n",
    "        for line in f:\n",
    "            if line.strip() == \"\":\n",
    "                break\n",
    "        # read the rest of the lines into a list\n",
    "        return [line.strip() for line in f]\n",
    "    # otherwise return an empty list\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4783 negative words\n",
      "There are 2006 positive words\n",
      "First 10 negative words:\n",
      "['2-faced', '2-faces', 'abnormal', 'abolish', 'abominable', 'abominably', 'abominate', 'abomination', 'abort', 'aborted']\n",
      "First 10 positive words:\n",
      "['a+', 'abound', 'abounds', 'abundance', 'abundant', 'accessable', 'accessible', 'acclaim', 'acclaimed', 'acclamation']\n"
     ]
    }
   ],
   "source": [
    "# load in the positive and negative word lists here\n",
    "# TODO: the paths to your negative/positive word files here\n",
    "neg_lex = load_word_list(\"negative_words.txt\")\n",
    "pos_lex = load_word_list(\"positive_words.txt\")\n",
    "\n",
    "# TODO: How many words are in each list?\n",
    "print(f\"There are {len(neg_lex)} negative words\")\n",
    "print(f\"There are {len(pos_lex)} positive words\")\n",
    "\n",
    "\n",
    "# TODO: Use python's list slicing to look at the first 10 elements in each list\n",
    "print(\"First 10 negative words:\")\n",
    "print(neg_lex[:10])\n",
    "\n",
    "print(\"First 10 positive words:\")\n",
    "print(pos_lex[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "envious\n",
      "enviously\n",
      "enviousness\n"
     ]
    }
   ],
   "source": [
    "# TODO: which words are in both the positive and the negative lists?\n",
    "for neg_word in neg_lex:\n",
    "    if neg_word in pos_lex:\n",
    "        print(neg_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll create our rule-based classifier! We have access to the word lists that you loaded and anything else you know about the world (reflect on how you as a human being can tell if a review is positive/negative). Your classifier need not be perfect, but it should be reasonable (don't just say everything is positive!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rule_based_classify(tokens, pos_lexicon, neg_lexicon, verbose = False):\n",
    "    \"\"\"\n",
    "    This function classifies a given tokenized text as positive or negative\n",
    "    based on the provided lexicons.\n",
    "    Parameters:\n",
    "    tokens (list) - list of strings tokenized words in the text to classify\n",
    "    pos_lexicon (list) - list of strings words in the positive word lexicon\n",
    "    neg_lexicon (list) - list of strings words in the negative word lexicon\n",
    "    verbose (boolean) - flag indicating whether or not to print verbose (debugging) output. \n",
    "            Default value False.\n",
    "    Returns:\n",
    "    string \"pos\" if the list of tokens is positive overall, \"neg\" if they are negative overall.\n",
    "    \"\"\"\n",
    "    score = 0\n",
    "    for token in tokens:\n",
    "        if token in pos_lexicon:\n",
    "            if verbose:\n",
    "                print(f\"Found positive word: {token}\")\n",
    "            score += 1\n",
    "        if token in neg_lexicon:\n",
    "            if verbose:\n",
    "                print(f\"Found negative word: {token}\")\n",
    "            score -= 1\n",
    "    if verbose:\n",
    "        print(f\"Score:\", score)\n",
    "    if score >= 0:\n",
    "        return \"pos\"\n",
    "    return \"neg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\ryan4\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative review classified as pos\n",
      "Positive review classified as pos\n"
     ]
    }
   ],
   "source": [
    "# now, we'll test out your classifier!\n",
    "# Here are two example movie reviews.\n",
    "nltk.download('movie_reviews')\n",
    "movies = corpus.movie_reviews\n",
    "\n",
    "# load in a single negative review\n",
    "negative_toks = movies.words('neg/cv001_19502.txt')\n",
    "# uncomment the text below to see the contents of the review\n",
    "neg_text = \" \".join(negative_toks)\n",
    "# print(neg_text)\n",
    "\n",
    "# load in a single positive review\n",
    "positive_toks = movies.words('pos/cv992_11962.txt')\n",
    "pos_text = \" \".join(positive_toks)\n",
    "\n",
    "\n",
    "# TODO:\n",
    "# call your rule_based_classify on these example reviews.\n",
    "print(\"Negative review classified as\", rule_based_classify(neg_text.split(), pos_lex, neg_lex, False))\n",
    "print(\"Positive review classified as\", rule_based_classify(pos_text.split(), pos_lex, neg_lex, False))\n",
    "\n",
    "# Does our classification function label them correctly? Why or why not?\n",
    "# take a look at the contents of the reviews\n",
    "# print(neg_text)\n",
    "# print(pos_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What labels does your classifier assign these two reviews? __The classifier assigns them both to be positive__\n",
    "2. Are these correct? __The negative review as labelled incorrectly, but the positive review was labelled correctly__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 3: How good is your sentiment classifier?\n",
    "-----\n",
    "\n",
    "Given the movies dataset from `nltk`, how many of the reviews does your classifier classify correctly?\n",
    "\n",
    "We'll look at three different metrics: __accuracy__, __precision__, and __recall__.\n",
    "\n",
    "__accuracy__: what you think of when you think of correctness.\n",
    "$$ \\frac{\\texttt{number correct}}{\\texttt{total number}}$$\n",
    "\n",
    "Precision and recall require differentiated between the ways in which the classifier can be correct or incorrect. \n",
    "\n",
    "- __true positive__: an example whose gold label is positive and that the classifier labels as positive\n",
    "- __true negative__: an example whose gold label is negative and that the classifier labels as negative\n",
    "- __false positive__: an example whose gold label is negative and that the classifier labels as positive\n",
    "- __false negative__: an example whose gold label is positive and that the classifier labels as negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# you can use numpy's random functionality if you'd like to\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "['neg/cv571_29292.txt', 'neg/cv701_15880.txt', 'neg/cv779_18989.txt', 'neg/cv205_9676.txt', 'neg/cv688_7884.txt', 'neg/cv378_21982.txt', 'neg/cv693_19147.txt', 'neg/cv531_26838.txt', 'neg/cv141_17179.txt', 'neg/cv848_10061.txt', 'neg/cv651_11120.txt', 'neg/cv633_29730.txt', 'neg/cv806_9405.txt', 'neg/cv650_15974.txt', 'neg/cv059_28723.txt', 'neg/cv287_17410.txt', 'neg/cv237_20635.txt', 'neg/cv209_28973.txt', 'neg/cv090_0049.txt', 'neg/cv652_15653.txt', 'neg/cv325_18330.txt', 'neg/cv879_16585.txt', 'neg/cv236_12427.txt', 'neg/cv830_5778.txt', 'neg/cv210_9557.txt', 'neg/cv614_11320.txt', 'neg/cv853_29119.txt', 'neg/cv756_23676.txt', 'neg/cv861_12809.txt', 'neg/cv615_15734.txt', 'neg/cv350_22139.txt', 'neg/cv424_9268.txt', 'neg/cv497_27086.txt', 'neg/cv730_10729.txt', 'neg/cv790_16202.txt', 'neg/cv835_20531.txt', 'neg/cv696_29619.txt', 'neg/cv659_21483.txt', 'neg/cv049_21917.txt', 'neg/cv052_29318.txt', 'neg/cv085_15286.txt', 'neg/cv604_23339.txt', 'neg/cv086_19488.txt', 'neg/cv479_5450.txt', 'neg/cv737_28733.txt', 'neg/cv528_11669.txt', 'neg/cv058_8469.txt', 'neg/cv386_10229.txt', 'neg/cv495_16121.txt', 'neg/cv883_27621.txt', 'neg/cv638_29394.txt', 'neg/cv912_5562.txt', 'neg/cv714_19704.txt', 'neg/cv518_14798.txt', 'neg/cv512_17618.txt', 'neg/cv581_20790.txt', 'neg/cv607_8235.txt', 'neg/cv108_17064.txt', 'neg/cv643_29282.txt', 'neg/cv761_13769.txt', 'neg/cv972_26837.txt', 'neg/cv583_29465.txt', 'neg/cv673_25874.txt', 'neg/cv043_16808.txt', 'neg/cv611_2253.txt', 'neg/cv218_25651.txt', 'neg/cv662_14791.txt', 'neg/cv453_10911.txt', 'neg/cv795_10291.txt', 'neg/cv676_22202.txt', 'neg/cv560_18608.txt', 'neg/cv131_11568.txt', 'neg/cv744_10091.txt', 'neg/cv768_12709.txt', 'neg/cv450_8319.txt', 'neg/cv670_2666.txt', 'neg/cv114_19501.txt', 'neg/cv775_17966.txt', 'neg/cv803_8584.txt', 'neg/cv864_3087.txt', 'neg/cv665_29386.txt', 'neg/cv901_11934.txt', 'neg/cv160_10848.txt', 'neg/cv543_5107.txt', 'neg/cv741_12765.txt', 'neg/cv782_21078.txt', 'neg/cv487_11058.txt', 'neg/cv112_12178.txt', 'neg/cv802_28381.txt', 'neg/cv578_16825.txt', 'neg/cv855_22134.txt', 'neg/cv395_11761.txt', 'neg/cv103_11943.txt', 'neg/cv258_5627.txt', 'neg/cv314_16095.txt', 'neg/cv129_18373.txt', 'neg/cv310_14568.txt', 'neg/cv831_16325.txt', 'neg/cv409_29625.txt', 'neg/cv130_18521.txt']\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "# To see the available file ids, this is one way that we can access them.\n",
    "# This will give you a list of neg/positive file ids.\n",
    "print(len(movies.fileids('neg')))\n",
    "# choose 100 random items without replacement from a list\n",
    "print(random.sample(movies.fileids('neg'), 100))\n",
    "print(len(movies.fileids('pos')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True positives: 67\n",
      "True negatives: 73\n",
      "False positives: 27\n",
      "False negatives: 33\n"
     ]
    }
   ],
   "source": [
    "# TODO:\n",
    "# Write code that uses your classifier to classify 100 randomly chosen\n",
    "# negative reviews and 100 randomly chosen positive reviews\n",
    "# count the number of true positives, true negatives, false positives, and false negatives\n",
    "\n",
    "# to get the tokens associated with a certain file id,\n",
    "# tokens = movies.words(file_id)\n",
    "\n",
    "# takes a long time to run if you loop over all fileids as opposed to just\n",
    "# 100 randomly chosen ones\n",
    "# make sure you don't classify the same review twice!\n",
    "# (it takes us about 10 seconds to classify 200 reviews on a 2020 macbook air)\n",
    "\n",
    "def classify_many_reviews(samples, review_type):\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    for sample in samples:\n",
    "        tokens = movies.words(sample)\n",
    "        result = rule_based_classify(tokens, pos_lex, neg_lex, False)\n",
    "        if result == \"pos\":\n",
    "            if review_type == \"pos\":\n",
    "                tp += 1\n",
    "            else:\n",
    "                fp += 1\n",
    "        elif result == \"neg\":\n",
    "            if review_type == \"neg\":\n",
    "                tn += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "    return tp, tn, fp, fn\n",
    "        \n",
    "neg_samples = random.sample(movies.fileids('neg'), 100)\n",
    "pos_samples = random.sample(movies.fileids('pos'), 100)\n",
    "    \n",
    "_, tn, fp, _ = classify_many_reviews(neg_samples, \"neg\")\n",
    "tp, _, _, fn = classify_many_reviews(pos_samples, \"pos\")\n",
    "    \n",
    "    \n",
    "# TODO: print out the number of true positives, false positives,\n",
    "# false negatives, and true negatives\n",
    "print(\"True positives:\", tp)\n",
    "print(\"True negatives:\", tn)\n",
    "print(\"False positives:\", fp)\n",
    "print(\"False negatives:\", fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the equations for accuracy, precision, and recall in terms of what we've just been counting. $tp$ means true positive, $fp$ means false positive, $fn$ means false negative, and $tn$ means true negative.\n",
    "\n",
    "$$ accuracy = \\frac{tp + tn}{tp + fp + fn + tn}$$\n",
    "\n",
    "$$ precision = \\frac{tp}{tp + fp}$$\n",
    "\n",
    "$$ recall = \\frac{tp}{tp + fn}$$\n",
    "\n",
    "You can think of precision as \"how many of my positive guesses were correct?\" and recall as \"how many of the positive examples did I find?\" ðŸ˜„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7\n"
     ]
    }
   ],
   "source": [
    "# TODO: calculate and print accuracy\n",
    "accuracy = (tp + tn)/(tp+fp+fn+tn)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7127659574468085\n"
     ]
    }
   ],
   "source": [
    "# TODO: calculate and print precision\n",
    "precision = tp/(tp+fp)\n",
    "print(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.67\n"
     ]
    }
   ],
   "source": [
    "# TODO: calculate and print recall\n",
    "recall = tp/(tp+fn)\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 4: n-gram math\n",
    "----\n",
    "\n",
    "Your final task in this lab is to do some math that will help you with your n-gram language model homework. Remember in HW 1 how you implemented a `count_list` function? Some of you were clever with how you implemented it, but let's look at a less clever implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That took: 0.1314988136291504 seconds!\n",
      "That took: 0.0 seconds!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "def count_list(ls: list) -> dict:\n",
    "    counts = {}\n",
    "    for item in ls:\n",
    "        # we're not going to be clever about counting here,\n",
    "        # no conditionals, no sets, nothing\n",
    "        counts[item] = ls.count(item)\n",
    "    return counts\n",
    "\n",
    "# see the difference between the following two items\n",
    "example = [random.randint(0, 100) for i in range(2000)]\n",
    "start = time.time()\n",
    "count_list(example)\n",
    "end = time.time()\n",
    "print(\"That took:\", end - start, \"seconds!\")\n",
    "\n",
    "# this takes a very similar amount of time to count_dict from HW 1\n",
    "start = time.time()\n",
    "Counter(example)\n",
    "end = time.time()\n",
    "print(\"That took:\", end - start, \"seconds!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: put your create_ngrams (or make_ngrams) function here!\n",
    "def make_ngrams(tokens: list, n: int) -> list:\n",
    "    \"\"\"Creates n-grams for the given token sequence.\n",
    "    Args:\n",
    "    tokens (list): a list of tokens as strings\n",
    "    n (int): the length of n-grams to create\n",
    "\n",
    "    Returns:\n",
    "    list: list of tuples of strings, each tuple being one of the individual n-grams\n",
    "    \"\"\"\n",
    "    ngrams = []\n",
    "    if n > len(tokens):\n",
    "        return ngrams\n",
    "    for token in range(len(tokens)-n+1):\n",
    "        temp = tuple(tokens[token:token+n])\n",
    "        ngrams.append(temp)\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final probability: 0.3333333333333333\n",
      "That took 0.0 seconds!\n"
     ]
    }
   ],
   "source": [
    "    # TODO: calculate the bigram score of the following sequence of tokens\n",
    "    # for this example, we'll use a \"vanilla\" scoring technique\n",
    "    # no Laplace smoothing, no unknown tokens\n",
    "    training_data = [\"<s>\", \"I\", \"love\", \"dogs\", \"</s>\", \"<s>\", \"I\", \"love\", \"cats\", \"</s>\", \"<s>\", \"I\", \"love\", \"dinosaurs\", \"</s>\"]\n",
    "\n",
    "    # TODO: call your create_ngrams function to get your bigrams\n",
    "\n",
    "    bigrams = make_ngrams(training_data, 2)\n",
    "\n",
    "    to_score = [\"<s>\", \"I\", \"love\", \"cats\", \"</s>\"]\n",
    "    start = time.time()\n",
    "\n",
    "    # BEGIN SCORING SECTION\n",
    "    # start probability at one so that we can multiply the probability of\n",
    "    # each subsequent next token with it\n",
    "    total_prob = 1\n",
    "    \n",
    "    training_bigram_counts = Counter(bigrams)\n",
    "    to_score_bigrams = make_ngrams(to_score, 2)\n",
    "    \n",
    "    #Chat-gpt revised this portion\n",
    "    first_token_counts = {}\n",
    "    for bigram in training_bigram_counts:\n",
    "        if bigram[0] in first_token_counts:\n",
    "            first_token_counts[bigram[0]] += training_bigram_counts[bigram]\n",
    "        else:\n",
    "            first_token_counts[bigram[0]] = training_bigram_counts[bigram]\n",
    "    #End\n",
    "\n",
    "    for i in range(1, len(to_score)):\n",
    "        bigram = to_score_bigrams[i-1]\n",
    "        if bigram not in training_bigram_counts:\n",
    "            total_prob = 0\n",
    "            pass\n",
    "        numerator = training_bigram_counts[bigram]\n",
    "        denom = first_token_counts[bigram[0]]\n",
    "        this_prob = numerator / denom\n",
    "        total_prob *= this_prob\n",
    "\n",
    "\n",
    "    # END SCORING SECTION\n",
    "    end = time.time()\n",
    "\n",
    "    # print your final probability\n",
    "    print(\"Final probability:\", total_prob)\n",
    "    print(\"That took\", end - start, \"seconds!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training tokens: 56670\n",
      "That took 0.06786227226257324 seconds!\n"
     ]
    }
   ],
   "source": [
    "# Finally, pretend that we had a lot more data\n",
    "training_data = [\"<s>\", \"I\", \"love\", \"dogs\", \"</s>\", \"<s>\", \"I\", \"love\", \"cats\", \"</s>\", \"<s>\", \"I\", \"love\", \"dinosaurs\", \"</s>\"]\n",
    "# this is the amount of training data in the berp set\n",
    "training_data = training_data * 3778\n",
    "\n",
    "# TODO: call your create_ngrams function here\n",
    "\n",
    "\n",
    "print(\"Number of training tokens:\", len(training_data))\n",
    "start = time.time()\n",
    "# and what if we had 5000 sentences to score?\n",
    "for example_num in range(3000):\n",
    "    total_prob = 1\n",
    "    \n",
    "    training_bigram_counts = Counter(bigrams)\n",
    "    to_score_bigrams = make_ngrams(to_score, 2)\n",
    "    \n",
    "    #Chat-gpt revised this portion\n",
    "    first_token_counts = {}\n",
    "    for bigram in training_bigram_counts:\n",
    "        if bigram[0] in first_token_counts:\n",
    "            first_token_counts[bigram[0]] += training_bigram_counts[bigram]\n",
    "        else:\n",
    "            first_token_counts[bigram[0]] = training_bigram_counts[bigram]\n",
    "    #End\n",
    "\n",
    "    for i in range(1, len(to_score)):\n",
    "        bigram = to_score_bigrams[i-1]\n",
    "        if bigram not in training_bigram_counts:\n",
    "            total_prob = 0\n",
    "            pass\n",
    "        numerator = training_bigram_counts[bigram]\n",
    "        denom = first_token_counts[bigram[0]]\n",
    "        this_prob = numerator / denom\n",
    "        total_prob *= this_prob \n",
    "\n",
    "end = time.time()\n",
    "print(\"That took\", end - start, \"seconds!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the moral of the story? If you perform your counts at the same time you score, you'll be doing the same work over and over again which will result in a significantly slower model!\n",
    "\n",
    "Make sure that you're gathering the counts that you need in `train` and only performing scoring calculations (as opposed to also counting things) in `score`.\n",
    "\n",
    "This is particularly important when using larger data sets! (berp is not that big)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
